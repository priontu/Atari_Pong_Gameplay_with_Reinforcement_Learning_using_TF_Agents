{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0569268e-6feb-4ce6-b042-6abfd73a9f95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-14 20:17:40.928419: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-14 20:17:41.092307: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-12-14 20:17:41.816403: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2022-12-14 20:17:41.816504: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2022-12-14 20:17:41.816513: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "A.L.E: Arcade Learning Environment (version 0.7.5+db37282)\n",
      "[Powered by Stella]\n",
      "2022-12-14 20:17:45.403713: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-14 20:17:45.414818: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-14 20:17:45.416487: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-14 20:17:45.418684: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-14 20:17:45.419597: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-14 20:17:45.421303: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-14 20:17:45.422964: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-14 20:17:46.098433: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-14 20:17:46.100351: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-14 20:17:46.101967: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-14 20:17:46.103492: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13584 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
      "2022-12-14 20:17:46.995680: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8200\n",
      "2022-12-14 20:17:48.299570: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 2822400000 exceeds 10% of free system memory.\n",
      "INFO:absl: \n",
      "\t\t NumberOfEpisodes = 0\n",
      "\t\t EnvironmentSteps = 0\n",
      "\t\t AverageReturn = 0.0\n",
      "\t\t AverageEpisodeLength = 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py:377: ReplayBuffer.get_next (from tf_agents.replay_buffers.replay_buffer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `as_dataset(..., single_deterministic_pass=False) instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py:377: ReplayBuffer.get_next (from tf_agents.replay_buffers.replay_buffer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `as_dataset(..., single_deterministic_pass=False) instead.\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "{{function_node __wrapped__IteratorGetNext_output_types_8_device_/job:localhost/replica:0/task:0/device:CPU:0}} assertion failed: [TFUniformReplayBuffer is empty. Make sure to add items before sampling the buffer.] [Condition x > y did not hold element-wise:] [x (TFUniformReplayBuffer/get_next/SelectV2_1:0) = ] [0] [y (TFUniformReplayBuffer/get_next/SelectV2:0) = ] [0]\n\t [[{{function_node TFUniformReplayBuffer_get_next_assert_greater_Assert_AssertGuard_false_463}}{{node TFUniformReplayBuffer/get_next/assert_greater/Assert/AssertGuard/Assert}}]] [Op:IteratorGetNext]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_24245/1207998232.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0msample_batch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m     \u001b[0mnum_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m     single_deterministic_pass=False)))\n\u001b[0m\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[0mtime_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_time_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_transition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrajectories\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    764\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 766\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    767\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_next_internal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    750\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    751\u001b[0m           \u001b[0moutput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flat_output_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 752\u001b[0;31m           output_shapes=self._flat_output_shapes)\n\u001b[0m\u001b[1;32m    753\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    754\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36miterator_get_next\u001b[0;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[1;32m   3015\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3016\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3017\u001b[0;31m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3018\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3019\u001b[0m       \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   7207\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7208\u001b[0m   \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 7209\u001b[0;31m   \u001b[0;32mraise\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   7210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: {{function_node __wrapped__IteratorGetNext_output_types_8_device_/job:localhost/replica:0/task:0/device:CPU:0}} assertion failed: [TFUniformReplayBuffer is empty. Make sure to add items before sampling the buffer.] [Condition x > y did not hold element-wise:] [x (TFUniformReplayBuffer/get_next/SelectV2_1:0) = ] [0] [y (TFUniformReplayBuffer/get_next/SelectV2:0) = ] [0]\n\t [[{{function_node TFUniformReplayBuffer_get_next_assert_greater_Assert_AssertGuard_false_463}}{{node TFUniformReplayBuffer/get_next/assert_greater/Assert/AssertGuard/Assert}}]] [Op:IteratorGetNext]"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "# TensorFlow ≥2.0 is required\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "assert tf.__version__ >= \"2.0\"\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "import PIL\n",
    "import jupyter_beeper\n",
    "import time\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# To get smooth animations\n",
    "import matplotlib.animation as animation\n",
    "mpl.rc('animation', html='jshtml')\n",
    "import tf_agents.environments.wrappers\n",
    "\n",
    "import gym\n",
    "# gym.envs.registry.all()\n",
    "\n",
    "from tf_agents.environments.wrappers import ActionRepeat\n",
    "\n",
    "game_name = \"Pong-v4\"\n",
    "\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"rl\"\n",
    "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n",
    "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
    "   \n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "from tf_agents.environments import suite_gym\n",
    "import ale_py\n",
    "\n",
    "from functools import partial\n",
    "from gym.wrappers import TimeLimit\n",
    "\n",
    "from tf_agents.environments import suite_atari\n",
    "from tf_agents.environments.atari_preprocessing import AtariPreprocessing\n",
    "from tf_agents.environments.atari_wrappers import FrameStack4\n",
    "from tf_agents.environments.tf_py_environment import TFPyEnvironment\n",
    "from tf_agents.networks.q_network import QNetwork\n",
    "from tf_agents.agents.dqn.dqn_agent import DqnAgent\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.metrics import tf_metrics\n",
    "from tf_agents.eval.metric_utils import log_metrics\n",
    "from tf_agents.drivers.dynamic_step_driver import DynamicStepDriver\n",
    "from tf_agents.policies.random_tf_policy import RandomTFPolicy\n",
    "from tf_agents.trajectories.trajectory import to_transition\n",
    "from tf_agents.utils.common import function\n",
    "from tf_agents.utils import common\n",
    "from tf_agents.policies import policy_saver\n",
    "\n",
    "import logging\n",
    "\n",
    "env = gym.make(game_name)\n",
    "# print(env)\n",
    "\n",
    "env.seed(42)\n",
    "env.reset()\n",
    "\n",
    "# env.step(1) # Fire\n",
    "repeating_env = ActionRepeat(env, times=4)\n",
    "repeating_env.unwrapped\n",
    "\n",
    "# for name in dir(tf_agents.environments.wrappers):\n",
    "#     obj = getattr(tf_agents.environments.wrappers, name)\n",
    "#     if hasattr(obj, \"__base__\") and issubclass(obj, tf_agents.environments.wrappers.PyEnvironmentBaseWrapper):\n",
    "#         print(\"{:27s} {}\".format(name, obj.__doc__.split(\"\\n\")[0]))\n",
    "\n",
    "        \n",
    "limited_repeating_env = suite_gym.load(\n",
    "    game_name,\n",
    "    gym_env_wrappers=[partial(TimeLimit, max_episode_steps=100)],\n",
    "    env_wrappers=[partial(ActionRepeat, times=4)],\n",
    ")\n",
    "\n",
    "max_episode_steps = 27000 # <=> 108k ALE frames since 1 step = 4 frames\n",
    "environment_name = \"PongNoFrameskip-v4\"\n",
    "\n",
    "# class AtariPreprocessingWithSkipStart(AtariPreprocessing):\n",
    "#     def skip_frames(self, num_skip):\n",
    "#         for _ in range(num_skip):\n",
    "#           super().step(0) # NOOP for num_skip steps\n",
    "#     def reset(self, **kwargs):\n",
    "#         obs = super().reset(**kwargs)\n",
    "#         self.skip_frames(40)\n",
    "#         return obs\n",
    "#     def step(self, action):\n",
    "#         lives_before_action = self.ale.lives()\n",
    "#         obs, rewards, done, info = super().step(action)\n",
    "#         if self.ale.lives() < lives_before_action and not done:\n",
    "#             self.skip_frames(40)\n",
    "#         return obs, rewards, done, info\n",
    "    \n",
    "class ShowProgress:\n",
    "    def __init__(self, total):\n",
    "        self.counter = 0\n",
    "        self.total = total\n",
    "    def __call__(self, trajectory):\n",
    "        if not trajectory.is_boundary():\n",
    "            self.counter += 1\n",
    "        if self.counter % 100 == 0:\n",
    "            print(\"\\r{}/{}\".format(self.counter, self.total), end=\"\")\n",
    "            \n",
    "def update_scene(num, frames, patch):\n",
    "    patch.set_data(frames[num])\n",
    "    return patch,\n",
    "\n",
    "def plot_animation(frames, repeat=False, interval=40):\n",
    "    fig = plt.figure()\n",
    "    patch = plt.imshow(frames[0])\n",
    "    plt.axis('off')\n",
    "    anim = animation.FuncAnimation(\n",
    "        fig, update_scene, fargs=(frames, patch),\n",
    "        frames=len(frames), repeat=repeat, interval=interval)\n",
    "    plt.close()\n",
    "    return anim\n",
    "\n",
    "def create_gif(frames):\n",
    "    gif_name = \"myAgentPlays-4.gif\" \n",
    "    image_path = os.path.join(\"images\", \"rl\", gif_name)\n",
    "    # frame_images = [PIL.Image.fromarray(frame) for frame in frames[:150]]\n",
    "    frame_images = [PIL.Image.fromarray(frame) for frame in frames]\n",
    "    frame_images[0].save(image_path, format='GIF',\n",
    "                         append_images=frame_images[1:],\n",
    "                         save_all=True,\n",
    "                         duration=30,\n",
    "                         loop=0)\n",
    "\n",
    "def train_agent(n_iterations):\n",
    "    time_step = None\n",
    "    policy_state = agent.collect_policy.get_initial_state(tf_env.batch_size)\n",
    "    iterator = iter(dataset)\n",
    "    for iteration in range(n_iterations):\n",
    "        time_step, policy_state = collect_driver.run(time_step, policy_state)\n",
    "        trajectories, buffer_info = next(iterator)\n",
    "        train_loss = agent.train(trajectories)\n",
    "        print(\"\\r{} loss:{:.5f}\".format(\n",
    "            iteration, train_loss.loss.numpy()), end=\"\")\n",
    "        if iteration % 1000 == 0:\n",
    "            log_metrics(train_metrics)\n",
    "            # train_checkpointer.save(agent.train_step_counter)\n",
    "        # print(\"iteration: \", iteration)\n",
    "\n",
    "env = suite_atari.load(\n",
    "    environment_name,\n",
    "    max_episode_steps=max_episode_steps,\n",
    "    gym_env_wrappers=[AtariPreprocessing, FrameStack4])\n",
    "\n",
    "env.seed(42)\n",
    "env.reset()\n",
    "# for _ in range(4):\n",
    "#     time_step = env.step(3) # LEFT\n",
    "\n",
    "# Building the Agent:\n",
    "tf_env = TFPyEnvironment(env)\n",
    "preprocessing_layer = keras.layers.Lambda(\n",
    "                          lambda obs: tf.cast(obs, np.float32) / 255.)\n",
    "conv_layer_params=[(32, (8, 8), 4), (64, (4, 4), 2), (64, (3, 3), 1)]\n",
    "fc_layer_params=[512]\n",
    "\n",
    "q_net = QNetwork(\n",
    "    tf_env.observation_spec(),\n",
    "    tf_env.action_spec(),\n",
    "    preprocessing_layers=preprocessing_layer,\n",
    "    conv_layer_params=conv_layer_params,\n",
    "    fc_layer_params=fc_layer_params)\n",
    "\n",
    "\n",
    "train_step = tf.Variable(0) # globak_step substitute\n",
    "update_period = 4 # run a training step every 4 collect steps\n",
    "optimizer = keras.optimizers.RMSprop(learning_rate=2.5e-4, rho=0.95, momentum=0.0,\n",
    "                                     epsilon=0.00001, centered=True)\n",
    "epsilon_fn = keras.optimizers.schedules.PolynomialDecay(\n",
    "    initial_learning_rate=1.0, # initial ε\n",
    "    decay_steps=25000// update_period, # <=> 1,000,000 ALE frames\n",
    "    end_learning_rate=0.01) # final ε\n",
    "\n",
    "agent = DqnAgent(tf_env.time_step_spec(),\n",
    "                 tf_env.action_spec(),\n",
    "                 q_network=q_net,\n",
    "                 optimizer=optimizer,\n",
    "                 target_update_period=2000, # <=> 32,000 ALE frames\n",
    "                 td_errors_loss_fn=keras.losses.Huber(reduction=\"none\"),\n",
    "                 gamma=0.99, # discount factor\n",
    "                 train_step_counter=train_step,\n",
    "                 epsilon_greedy=lambda: epsilon_fn(train_step))\n",
    "agent.initialize()\n",
    "\n",
    "# Building the Replay Buffer:\n",
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "    data_spec=agent.collect_data_spec,\n",
    "    batch_size=tf_env.batch_size,\n",
    "    max_length=100000) # reduce if OOM error\n",
    "\n",
    "checkpoint_dir = os.path.join(os.getcwd(), 'lastModelCheckpoint')\n",
    "train_checkpointer = common.Checkpointer(\n",
    "    ckpt_dir=checkpoint_dir,\n",
    "    max_to_keep=1,\n",
    "    agent=agent,\n",
    "    policy=agent.policy,\n",
    "    replay_buffer=replay_buffer,\n",
    "    global_step=train_step\n",
    ")\n",
    "\n",
    "train_checkpointer.initialize_or_restore()\n",
    "global_step = tf.compat.v1.train.get_global_step()\n",
    "\n",
    "\n",
    "replay_buffer_observer = replay_buffer.add_batch\n",
    "\n",
    "# Creating Train Metrics:\n",
    "train_metrics = [\n",
    "    tf_metrics.NumberOfEpisodes(),\n",
    "    tf_metrics.EnvironmentSteps(),\n",
    "    tf_metrics.AverageReturnMetric(),\n",
    "    tf_metrics.AverageEpisodeLengthMetric(),\n",
    "]\n",
    "\n",
    "\n",
    "# Log Metrics:\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "log_metrics(train_metrics)\n",
    "\n",
    "collect_driver = DynamicStepDriver(\n",
    "    tf_env,\n",
    "    agent.collect_policy,\n",
    "    observers=[replay_buffer_observer] + train_metrics,\n",
    "    num_steps=update_period) # collect 4 steps for each training iteration\n",
    "\n",
    "\n",
    "trajectories, buffer_info = next(iter(replay_buffer.as_dataset(\n",
    "    sample_batch_size=2,\n",
    "    num_steps=3,\n",
    "    single_deterministic_pass=False)))\n",
    "\n",
    "time_steps, action_steps, next_time_steps = to_transition(trajectories)\n",
    "\n",
    "dataset = replay_buffer.as_dataset(\n",
    "    sample_batch_size=64,\n",
    "    num_steps=2,\n",
    "    num_parallel_calls=3).prefetch(3)\n",
    "\n",
    "collect_driver.run = function(collect_driver.run)\n",
    "agent.train = function(agent.train)\n",
    "\n",
    "start_time = time.time()\n",
    "train_agent(n_iterations=100)\n",
    "print(\"\\nTime Taken: \", time.time() - start_time)\n",
    "# beep_when_finished()\n",
    "\n",
    "eval_policy = agent.policy\n",
    "# eval_policy_dir = os.path.join(os.getcwd(), 'eval_policy')\n",
    "# policy_dir = os.path.join(os.getcwd(), 'savedPolicy')\n",
    "# tf_policy_saver = policy_saver.PolicySaver(eval_policy)\n",
    "# tf_policy_saver.save(policy_dir)\n",
    "# tf_policy_saver.save(eval_policy_dir)\n",
    "# train_checkpointer.save(train_step)\n",
    "# checkpoint_path = os.path.join(os.getcwd(), \"savedModel\", \"cpkt\")\n",
    "# os.makedirs(checkpoint_path, exist_ok=True)\n",
    "# model_checkpoint = tf.train.Checkpoint(model = agent, step = train_step)\n",
    "# saved_path = model_checkpoint.save(file_prefix = checkpoint_path)\n",
    "# print(\"Model saced in: \\n\", saved_path)\n",
    "\n",
    "frames = []\n",
    "def save_frames(trajectory):\n",
    "    global frames\n",
    "    frames.append(tf_env.pyenv.envs[0].render(mode=\"rgb_array\"))\n",
    "\n",
    "num_frames = 1000\n",
    "    \n",
    "watch_driver = DynamicStepDriver(\n",
    "    tf_env,\n",
    "    agent.policy,\n",
    "    observers=[save_frames, ShowProgress(num_frames)],\n",
    "    num_steps=num_frames)\n",
    "final_time_step, final_policy_state = watch_driver.run()\n",
    "\n",
    "anim = plot_animation(frames)\n",
    "\n",
    "create_gif(frames)\n",
    "\n",
    "print(\"\\n------------Program Execution Complete-------------\\n\")\n",
    "\n",
    "anim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f77fa2-a5c3-4725-9def-947945372e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = []\n",
    "def save_frames(trajectory):\n",
    "    global frames\n",
    "    frames.append(tf_env.pyenv.envs[0].render(mode=\"rgb_array\"))\n",
    "\n",
    "num_frames = 1000\n",
    "    \n",
    "watch_driver = DynamicStepDriver(\n",
    "    tf_env,\n",
    "    agent.policy,\n",
    "    observers=[save_frames, ShowProgress(num_frames)],\n",
    "    num_steps=num_frames)\n",
    "final_time_step, final_policy_state = watch_driver.run()\n",
    "\n",
    "anim = plot_animation(frames)\n",
    "\n",
    "create_gif(frames)\n",
    "\n",
    "print(\"\\n------------Program Execution Complete-------------\\n\")\n",
    "\n",
    "anim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af9e753-6436-4e0e-b22c-de99ad2b5135",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_policy = agent.policy\n",
    "eval_policy_dir = os.path.join(os.getcwd(), 'eval_policy')\n",
    "policy_dir = os.path.join(os.getcwd(), 'savedPolicy')\n",
    "tf_policy_saver = policy_saver.PolicySaver(eval_policy)\n",
    "tf_policy_saver.save(policy_dir)\n",
    "tf_policy_saver.save(eval_policy_dir)\n",
    "train_checkpointer.save(train_step)\n",
    "checkpoint_path = os.path.join(os.getcwd(), \"savedModel\", \"cpkt\")\n",
    "os.makedirs(checkpoint_path, exist_ok=True)\n",
    "model_checkpoint = tf.train.Checkpoint(model = agent, step = train_step)\n",
    "saved_path = model_checkpoint.save(file_prefix = checkpoint_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6c785f-9f51-4337-bb79-8c5ee8bb10e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = os.path.join(os.getcwd(), \"savedModel\", \"cpkt\")\n",
    "os.makedirs(checkpoint_path, exist_ok=True)\n",
    "model_checkpoint = tf.train.Checkpoint(model = agent, step = train_step)\n",
    "saved_path = model_checkpoint.save(file_prefix = checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5784b25e-6b03-49be-b462-9661925a4b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6794371-6552-4f0a-8042-ce0d3af18252",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_metrics(train_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0049b2-4c05-40c0-ad1c-a27cace19b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6879344-9fb8-477b-b05f-213468927a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gif(frames):\n",
    "    gif_name = \"myAgentPlays-2_v0.gif\" \n",
    "    image_path = os.path.join(\"images\", \"rl\", gif_name)\n",
    "    # frame_images = [PIL.Image.fromarray(frame) for frame in frames[:150]]\n",
    "    frame_images = [PIL.Image.fromarray(frame) for frame in frames]\n",
    "    frame_images[0].save(image_path, format='GIF',\n",
    "                         append_images=frame_images[1:],\n",
    "                         save_all=True,\n",
    "                         duration=30,\n",
    "                         loop=0)\n",
    "\n",
    "\n",
    "create_gif(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb866d5-1f00-4ef2-ad83-0428cd646d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gif(frames):\n",
    "    gif_name = \"space_invaders_26_2_wins.gif\" \n",
    "    image_path = os.path.join(\"images\", \"rl\", gif_name)\n",
    "    # frame_images = [PIL.Image.fromarray(frame) for frame in frames[:150]]\n",
    "    frame_images = [PIL.Image.fromarray(frame) for frame in frames]\n",
    "    frame_images[0].save(image_path, format='GIF',\n",
    "                         append_images=frame_images[1:],\n",
    "                         save_all=True,\n",
    "                         duration=30,\n",
    "                         loop=0)\n",
    "    \n",
    "create_gif(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0077097f-d1b7-43fd-a533-20acef93c358",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "# gif_dir = \"images/rl/\" + gif_name\n",
    "<img src=\"images/rl/space_invaders_2M_16_good.gif\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb93e2e-108f-4314-bd16-b5ccd1cd7495",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_dir = os.path.join(os.getcwd(), 'policy')\n",
    "tf_policy_saver = policy_saver.PolicySaver(agent.policy)\n",
    "tf_policy_saver.save(policy_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081df154-cf11-4b63-9304-27214c7f8f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_policy = tf.saved_model.load(policy_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181caf64-7846-45a8-8e5d-676fb52d483c",
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = []\n",
    "def save_frames(trajectory):\n",
    "    global frames\n",
    "    frames.append(tf_env.pyenv.envs[0].render(mode=\"rgb_array\"))\n",
    "\n",
    "num_frames = 1000\n",
    "    \n",
    "watch_driver = DynamicStepDriver(\n",
    "    tf_env,\n",
    "    saved_policy,\n",
    "    observers=[save_frames, ShowProgress(num_frames)],\n",
    "    num_steps=num_frames)\n",
    "final_time_step, final_policy_state = watch_driver.run()\n",
    "\n",
    "plot_animation(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79009a94-d11b-4afa-b733-c331ce49c1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "gif_name = \"space_invaders_2M_17_2nd_win.gif\" \n",
    "\n",
    "image_path = os.path.join(\"images\", \"rl\", gif_name)\n",
    "# frame_images = [PIL.Image.fromarray(frame) for frame in frames[:150]]\n",
    "frame_images = [PIL.Image.fromarray(frame) for frame in frames]\n",
    "frame_images[0].save(image_path, format='GIF',\n",
    "                     append_images=frame_images[1:],\n",
    "                     save_all=True,\n",
    "                     duration=30,\n",
    "                     loop=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1122b652-539b-49d2-a117-766009b9e0ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-10.m100",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-10:m100"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
